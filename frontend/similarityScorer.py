# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11dSGlvjsG_hcDv2Yz5mf-3ykn8MBdsad

# Sentence modeling
- One of the methods to represent sentences as vectors (Mu et al 2017)
- Computing vector representations of each embedded word, and weight average them using PCA
    - If there are **n** words in a sentence, select **N** words with high explained variance (n>N)
    - Most of "energy" (around 80%) can be containted using only 4 words (N=4) in the original paper (Mu et al 2017)
"""

import re
import numpy as np
from gensim.models import Word2Vec
from nltk.corpus import gutenberg
from nltk.tokenize import word_tokenize
from multiprocessing import Pool
from scipy import spatial
from sklearn.decomposition import PCA
import nltk
nltk.download('gutenberg')
nltk.download('punkt')

def getSimilarityFunction(descriptions):
	sentences=[word_tokenize(des) for des in descriptions]

	for i in range(len(sentences)):
		sentences[i] = [word.lower() for word in sentences[i] if re.match('^[a-zA-Z]+', word)]

	model = Word2Vec(sentences = sentences, size = 100, sg = 1, window = 3, min_count = 1, iter = 10, workers = Pool()._processes)
	model.init_sims(replace = True)

	# converting each word into its vector representation
	for i in range(len(sentences)):
		sentences[i] = [model[word] for word in sentences[i]]

	# define function to compute weighted vector representation of sentence
	# parameter 'n' means number of words to be accounted when computing weighted average
	def sent_PCA(sentence, n = 3):
		pca = PCA(n_components = n)
		pca.fit(np.array(sentence).transpose())
		variance = np.array(pca.explained_variance_ratio_)
		words = []
		for _ in range(n):
			idx = np.argmax(variance)
			words.append(np.amax(variance) * sentence[idx])
			variance[idx] = 0
		return np.sum(words, axis = 0)

	def sentence_similarity(sent1,sent2):
		sent1=word_tokenize(sent1)
		sent2=word_tokenize(sent2)
		sent1=[word.lower() for word in sent1 if re.match('^[a-zA-Z]+', word)]
		sent2=[word.lower() for word in sent2 if re.match('^[a-zA-Z]+', word)]
		emb1=[model[word] for word in sent1]
		emb2=[model[word] for word in sent2]
		return 1 - spatial.distance.cosine(sent_PCA(emb1), sent_PCA(emb2))


	return sentence_similarity
	
	
descriptions=["Men Solid Casual Shirt","Classic Regular Fit Shirt","Regular Fit Casual Shirt"]

fun=getSimilarityFunction(descriptions)

out=fun(descriptions[1],descriptions[2])

print(out)

